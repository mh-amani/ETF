{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.utils.triplet_utils import TripletUtils\n",
    "\n",
    "import jsonlines\n",
    "import pickle\n",
    "import json \n",
    "import os\n",
    "\n",
    "mappings_folder = \"../data/id2name_mappings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mapping(mapping, output_dir, output_file_name):\n",
    "    data = [{\"id\": key, \"en_label\": value} for key, value in mapping.items() if key is not None]\n",
    "\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    with jsonlines.open(output_file_path, \"w\") as writer:\n",
    "        writer.write_all(data)\n",
    "\n",
    "\n",
    "path_to_entity_mapping = os.path.join(\"../data/rebel\", \"english_mapping.pickle\")\n",
    "with open(path_to_entity_mapping, \"rb\") as f:\n",
    "    ent_m = pickle.load(f)\n",
    "process_mapping(ent_m, mappings_folder, \"entity_mapping.jsonl\")\n",
    "\n",
    "\n",
    "path_to_relations_mapping = os.path.join(\"../data/rebel\", \"relations.pickle\")\n",
    "with open(path_to_relations_mapping, \"rb\") as f:\n",
    "    rel_m = pickle.load(f)\n",
    "process_mapping(rel_m, mappings_folder, \"relation_mapping.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_world(constrained_worlds_dir, world_id):\n",
    "    assert world_id in {None, \"genie\"}, \"Invalid constrained world identifier {}\".format(world_id)\n",
    "\n",
    "    if world_id is None:\n",
    "        return\n",
    "\n",
    "    path_to_constrained_world_dir = os.path.join(constrained_worlds_dir, world_id)\n",
    "    \n",
    "    with open(os.path.join(path_to_constrained_world_dir, \"entities.json\")) as json_file:\n",
    "        entities = json.load(json_file)\n",
    "\n",
    "    with open(os.path.join(path_to_constrained_world_dir, \"relations.json\")) as json_file:\n",
    "        relations = json.load(json_file)\n",
    "\n",
    "    return entities, relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~ Maximal relation set from training and maximal entity set from training + val + test ~~~~\n",
    "keep_spaces_entity = False\n",
    "keep_spaces_relation = True\n",
    "\n",
    "\n",
    "# Read entity mapping \n",
    "ent_mapping_path = os.path.join(mappings_folder, \"entity_mapping.jsonl\")\n",
    "with jsonlines.open(ent_mapping_path) as reader:\n",
    "    ent_map = {obj['id']: obj['en_label'] for obj in reader}\n",
    "# Read relation mapping\n",
    "rel_mapping_path = os.path.join(mappings_folder, \"relation_mapping.jsonl\")\n",
    "with jsonlines.open(rel_mapping_path) as reader:\n",
    "    rel_map = {obj['id']: obj['en_label'] for obj in reader}\n",
    "\n",
    "\n",
    "# Get entities and relations\n",
    "ent_ids, rel_ids = _read_world(\"../data/constrained_worlds\", \"genie\")\n",
    "\n",
    "# Map ids to names and normalize them\n",
    "ent_names = [TripletUtils.normalize_spaces(ent_map[_id], keep_spaces=keep_spaces_entity) for _id in ent_ids if _id in ent_map]\n",
    "rel_names = [TripletUtils.normalize_spaces(rel_map[_id], keep_spaces=keep_spaces_relation) for _id in rel_ids if _id in rel_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 2724925 entities were not found in the mapping\n",
      "0 out of 1088 entities were not found in the mapping\n"
     ]
    }
   ],
   "source": [
    "# valid_rebel_ent_ids = [ent_id for ent_id in ent_ids if ent_id in ent_map]\n",
    "\n",
    "# import json\n",
    "# with open('entities.json', 'w') as f:\n",
    "#     json.dump(valid_rebel_ent_ids, f)\n",
    "\n",
    "print(f\"{len(ent_ids) - len(ent_names)} out of {len(ent_ids)} entities were not found in the mapping\")\n",
    "print(f\"{len(rel_ids) - len(rel_names)} out of {len(rel_ids)} entities were not found in the mapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_linearization:\n",
    "#   formatted_triplet:\n",
    "#     [\n",
    "#       \"{subject_id}\",\n",
    "#       \" \",\n",
    "#       \"{subject}\",\n",
    "#       \" \",\n",
    "#       \"{relation_id}\",\n",
    "#       \" \",\n",
    "#       \"{relation}\",\n",
    "#       \" \",\n",
    "#       \"{object_id}\",\n",
    "#       \" \",\n",
    "#       \"{object}\",\n",
    "#       \" \",\n",
    "#       \"{et_id}\",\n",
    "#     ]\n",
    "#   keep_spaces: false\n",
    "#   surface_forms:\n",
    "#     \"{subject_id}\": \"[s]\"\n",
    "#     \"{relation_id}\": \"[r]\"\n",
    "#     \"{object_id}\": \"[o]\"\n",
    "#     \"{et_id}\": \"[et]\"\n",
    "#     \"{separator}\": \" \"\n",
    "\n",
    "# linearized_triplets.extend([surface_forms.get(item, item) for item in formatted_triplet])\n",
    "# linearized_triplets.append(surface_forms[\"{separator}\"])\n",
    "\n",
    "from transformers import T5Tokenizer, T5Config\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "config = T5Config.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "def encode(text, keep_eos: bool):\n",
    "    if keep_eos:\n",
    "        return tokenizer.encode(text)\n",
    "    \n",
    "    return tokenizer.encode(text)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([784, 7, 1], [3, 7, 1])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\" [s\"), tokenizer.encode(\" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[489, 4608, 1]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'['"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 7, 908, 1]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"[s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lambda x: x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.eleni = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eleni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_text = \"[s] Trinity_Peninsula [r] part of [o] Graham_Land [et] [s] Trinity_Peninsula [r] continent [o] Antarctica [et] [s] Graham_Land [r] continent [o] Antarctica [et]</s>\"\n",
    "target_ids = [784, 7, 908, # sub_id\n",
    "              20699, 834, 345, 35, 15953, 9,  # sub\n",
    "              784, 52, 908, # rel_id\n",
    "              294, 13, # rel\n",
    "              784, 32, 908, # obj_id\n",
    "              15146, 834, 434, 232, # obj\n",
    "              784, 15, 17, 908, # et_id\n",
    "              784, 7, 908,\n",
    "              20699, 834, 345, 35, 15953, 9, 784, 52, 908, 10829, 784, 32, 908, 26461, 9, 784, 15, 17, 908, 784, 7, 908, 15146, 834, 434, 232, 784, 52, 908, 10829, 784, 32, 908, 26461, 9, 784, 15, 17, 908, 1]\n",
    "# (784, 7, 908)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(0) # decoder_start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 15, 17, 908, 1]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"[et]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_allowed_tokens_fn_params = {\n",
    "    \"subject_token\": \"s\",\n",
    "    \"relation_token\": \"r\",\n",
    "    \"object_token\": \"o\",\n",
    "    \"end_of_triple_token\": \"et\",\n",
    "    \"start_of_triple_tag\": \"[\",\n",
    "    \"end_of_triple_tag\": \"]\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = [\"[s]\", \"[r]\", \"[o]\", \"[et]\"]\n",
    "\n",
    "# if inside identifier, return options\n",
    "\n",
    "# if ouside identifier, return options\n",
    "# codes = [entity trie, relation trie, entity trie, exit or start of [s]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "patterns_of_interest = [\"[s]\", \"[r]\", \"[o]\", \"[et]\"]\n",
    "\n",
    "def find_total_occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mlr/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:219: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[784,\n",
       " 7,\n",
       " 908,\n",
       " 20699,\n",
       " 834,\n",
       " 345,\n",
       " 35,\n",
       " 15953,\n",
       " 9,\n",
       " 784,\n",
       " 52,\n",
       " 908,\n",
       " 294,\n",
       " 13,\n",
       " 784,\n",
       " 32,\n",
       " 908,\n",
       " 15146,\n",
       " 834,\n",
       " 434,\n",
       " 232,\n",
       " 784,\n",
       " 15,\n",
       " 17,\n",
       " 908,\n",
       " 784,\n",
       " 7,\n",
       " 908,\n",
       " 20699,\n",
       " 834,\n",
       " 345,\n",
       " 35,\n",
       " 15953,\n",
       " 9,\n",
       " 784,\n",
       " 52,\n",
       " 908,\n",
       " 10829,\n",
       " 784,\n",
       " 32,\n",
       " 908,\n",
       " 26461,\n",
       " 9,\n",
       " 784,\n",
       " 15,\n",
       " 17,\n",
       " 908,\n",
       " 784,\n",
       " 7,\n",
       " 908,\n",
       " 15146,\n",
       " 834,\n",
       " 434,\n",
       " 232,\n",
       " 784,\n",
       " 52,\n",
       " 908,\n",
       " 10829,\n",
       " 784,\n",
       " 32,\n",
       " 908,\n",
       " 26461,\n",
       " 9,\n",
       " 784,\n",
       " 15,\n",
       " 17,\n",
       " 908,\n",
       " 1]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(target_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[s]',\n",
       " 'Trinity_Peninsula',\n",
       " '[r]',\n",
       " 'part of',\n",
       " '[o]',\n",
       " 'Graham_Land',\n",
       " '[et]',\n",
       " '[s]')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tokenizer.decode([784, 7, 908]), \n",
    "tokenizer.decode([20699, 834, 345, 35, 15953, 9]), \n",
    "tokenizer.decode([784, 52, 908]), \n",
    "tokenizer.decode([294, 13]),\n",
    "tokenizer.decode([784, 32, 908]),\n",
    "tokenizer.decode([15146, 834, 434, 232]),\n",
    "tokenizer.decode([784, 15, 17, 908]),\n",
    "tokenizer.decode([784, 7, 908])) # And so on until you close the triplet with 784, 15, 17, 908 and then 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15146, 834, 434, 232, 1]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"Graham_Land\", keep_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[s]\n",
      "[r]\n",
      "[o]\n",
      "[et]\n"
     ]
    }
   ],
   "source": [
    "special_tokens_text = {\n",
    "    \"sub_id\": \"s\",\n",
    "    \"rel_id\": \"r\",\n",
    "    \"obj_id\": \"o\",\n",
    "    \"et_id\": \"et\",\n",
    "}\n",
    "\n",
    "full_identifiers = {\"sub_id\": \"[s]\", \"rel_id\": \"[r]\", \"obj_id\": \"[o]\", \"et_id\": \"[et]\"}\n",
    "\n",
    "# verify that tokenized partial and full identifiers are eqivalent\n",
    "for key, value in full_identifiers.items():\n",
    "    print(value)\n",
    "    assert tokenizer.encode(value)[:-1] == tokenizer.encode(f\"{special_tokens_text['start_of_tag']}{special_tokens_text[key]}{special_tokens_text['end_of_tag']}\")[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub_id': 's',\n",
       " 'rel_id': 'r',\n",
       " 'obj_id': 'o',\n",
       " 'et_id': 'et',\n",
       " 'start_of_tag': '[',\n",
       " 'end_of_tag': ']'}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 7, 908, 188, 784, 7, 908, 71, 1]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"[s]A [s] A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('toA', 'to A')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([12, 188]), tokenizer.decode([12, 71], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[s]A [s] A</s>'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"[s]A [s] A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub_id': array([784,   7, 908]),\n",
       " 'rel_id': array([784,  52, 908]),\n",
       " 'obj_id': array([784,  32, 908]),\n",
       " 'et_id': array([784,  15,  17, 908])}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_text = {\n",
    "    \"sub_id\": \"[s]\",\n",
    "    \"rel_id\": \"[r]\",\n",
    "    \"obj_id\": \"[o]\",\n",
    "    \"et_id\": \"[et]\",\n",
    "}\n",
    "\n",
    "special_tokens_ids = {key: np.array(encode(value, keep_eos=False)) for key, value in special_tokens_text.items()}\n",
    "special_tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ids = np.array([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def encode(text, keep_eos: bool):\n",
    "    if keep_eos:\n",
    "        return tokenizer.encode(text)\n",
    "    \n",
    "    return tokenizer.encode(text)[:-1]\n",
    "\n",
    "def get_prefix_allowed_tokens_fn(model, entities_trie, relation_trie, sub_id_token = \"[s]\", rel_id_token = \"[r]\", obj_id_token = \"[o]\", et_id_token = \"[et]\"):\n",
    "    EOS_TOKEN = model.tokenizer.eos_token_id\n",
    "\n",
    "    state_id2token_ids = {'sub_id': np.array(encode(sub_id_token, keep_eos=False)),\n",
    "                            'rel_id': np.array(encode(rel_id_token, keep_eos=False)),\n",
    "                            'obj_id': np.array(encode(obj_id_token, keep_eos=False)),\n",
    "                            'et_id': np.array(encode(et_id_token, keep_eos=False)),\n",
    "                            }\n",
    "\n",
    "    state_id2next_state_id = {\"sub_id\": \"rel_id\", \"rel_id\": \"obj_id\", \"obj_id\": \"et_id\", \"et_id\": \"sub_id\"}\n",
    "\n",
    "    def _get_next_state_id(state_id):\n",
    "        return state_id2token_ids[state_id2next_state_id[state_id]]\n",
    "\n",
    "    def _get_allowed_tokens_from_trie(suffix, trie, next_state_first_token_id):\n",
    "        allowed_tokens = trie.get(suffix)\n",
    "\n",
    "        if EOS_TOKEN in allowed_tokens:\n",
    "            allowed_tokens.remove(EOS_TOKEN)\n",
    "            allowed_tokens.append(next_state_first_token_id)\n",
    "\n",
    "        return allowed_tokens\n",
    "\n",
    "    def get_allowed_tokens(state_id, suffix):\n",
    "        next_state_id = _get_next_state_id(state_id)\n",
    "\n",
    "        # ~~~ if currently generating a state identifier ~~~\n",
    "        while next_state_id.size > 1:\n",
    "            window = next_state_id[:-1]\n",
    "\n",
    "            if suffix.size < window.size:\n",
    "                next_state_id = window\n",
    "                continue \n",
    "\n",
    "            if np.array_equal(window, suffix[-len(window):]):\n",
    "                return [next_state_id[-1]]\n",
    "                \n",
    "            next_state_id = window\n",
    "        \n",
    "        # ~~~ otherwise ~~~\n",
    "        if state_id == \"et_id\":\n",
    "            return [EOS_TOKEN, _get_next_state_id(state_id)[0]]\n",
    "\n",
    "        elif state_id == \"rel_id\":\n",
    "            return _get_allowed_tokens_from_trie(suffix, relation_trie, _get_next_state_id(state_id)[0])\n",
    "\n",
    "        return _get_allowed_tokens_from_trie(suffix, entities_trie, _get_next_state_id(state_id)[0])\n",
    "\n",
    "    def _get_state_id_and_suffix_start(sent_ids):\n",
    "        last_token_idx_plus_one = len(sent_ids)\n",
    "\n",
    "        while last_token_idx_plus_one > 0:\n",
    "            for state_id, pattern in state_id2token_ids.items():\n",
    "                pat_size = pattern.size\n",
    "\n",
    "                if last_token_idx_plus_one < pat_size:\n",
    "                    continue\n",
    "\n",
    "                window = sent_ids[last_token_idx_plus_one-pat_size:last_token_idx_plus_one] \n",
    "                if np.array_equal(window, pattern):\n",
    "                    return state_id, last_token_idx_plus_one\n",
    "\n",
    "            last_token_idx_plus_one -= 1\n",
    "        \n",
    "        return \"et_id\", 0\n",
    "\n",
    "    def prefix_allowed_tokens_fn(batch_id: int, sent_ids: torch.Tensor) -> Iterable[int]:\n",
    "        sent_ids = sent_ids.tonumpy()\n",
    "\n",
    "        # ToDo: Is this necessary? It was for genie for some weird reason that I didn't figure out\n",
    "        if len(sent_ids) > 1 and sent_ids[-1] == EOS_TOKEN:\n",
    "            return []\n",
    "\n",
    "        state_id, suffix = _get_state_id_and_suffix_start(sent_ids)\n",
    "        return get_allowed_tokens(state_id, suffix)\n",
    "\n",
    "    return prefix_allowed_tokens_fn\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "sent_ids = np.array([0, 1, 2, 3, 4])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub_id': array([784,   7, 908]),\n",
       " 'rel_id': array([784,  52, 908]),\n",
       " 'obj_id': array([784,  32, 908]),\n",
       " 'et_id': array([784,  15,  17, 908])}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = special_tokens_ids.copy()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_ids = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "temp = special_tokens_ids.copy()\n",
    "\n",
    "# Case 1\n",
    "temp[\"test\"] = np.array([2, 3])\n",
    "sc, idx = get_state_and_suffix_start(sent_ids, temp)\n",
    "assert sc == \"test\" and idx == 3\n",
    "\n",
    "# Case 2\n",
    "temp[\"older\"] = np.array([1, 2])\n",
    "sc, idx = get_state_and_suffix_start(sent_ids, temp)\n",
    "assert sc == \"test\" and idx == 3\n",
    "\n",
    "# Case 3\n",
    "temp[\"newer\"] = np.array([3, 4])\n",
    "sc, idx = get_state_and_suffix_start(sent_ids, temp)\n",
    "assert sc == \"newer\" and idx == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix = sent_ids[idx:]\n",
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test', 3)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "constrained_worlds_dir = os.path.join(\"../data\", \"constrained_worlds\")\n",
    "world_id = \"genie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities, relations = _read_world(constrained_worlds_dir, world_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q1607497', 'Q22958416', 'Q1848116', 'Q1513683', 'Q19880251']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in english_mapping.items():\n",
    "    if key is not None and key.startswith(\"P\"):\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in english_mapping.items():\n",
    "    if key is not None and key.startswith(\"P\"):\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "en_map = [{\"id\": key, \"en_label\": value} for key, value in ent_m.items() if key is not None]\n",
    "\n",
    "folder = \"../data/id2name_mappings\"\n",
    "file_name = \"entity_mapping.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "en_map = [{\"id\": key, \"en_label\": value} for key, value in ent_m.items() if key is not None]\n",
    "\n",
    "folder = \"../data/id2name_mappings\"\n",
    "file_name = \n",
    "\n",
    "output_file_path = os.path.join(folder, file_name)\n",
    "with jsonlines.open(output_file_path, \"w\") as writer:\n",
    "    writer.write_all(en_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Herangi_Range'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_mapping[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q191069', 'Q47716', 'Q15643', 'Q2143143', 'Q1899035']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(keys)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n[Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61a29ff56f295d5304320f1d8bf976f57364ea8d91ff2512104112c4617b5d63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
